import os
import logging
import asyncio
import json
import numpy as np
import pandas as pd
import yfinance as yf
import ccxt
import sqlite3
from datetime import datetime, timedelta
import pytz
import io
import base64
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, CallbackQueryHandler, filters, ContextTypes
from dotenv import load_dotenv

load_dotenv()

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø´Ø±Ø·ÛŒ
try:
    import torch
    import transformers
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    TORCH_AVAILABLE = True
    logger.info("PyTorch and Transformers loaded successfully")
except ImportError as e:
    TORCH_AVAILABLE = False
    logger.warning(f"PyTorch or Transformers not available: {e}")

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    TF_AVAILABLE = True
    logger.info("TensorFlow loaded successfully")
except ImportError as e:
    TF_AVAILABLE = False
    logger.warning(f"TensorFlow not available: {e}")

try:
    import talib
    TALIB_AVAILABLE = True
    logger.info("TA-Lib loaded successfully")
except ImportError as e:
    TALIB_AVAILABLE = False
    logger.warning(f"TA-Lib not available: {e}")

try:
    import pandas_ta as ta
    PANDAS_TA_AVAILABLE = True
    logger.info("pandas-ta loaded successfully")
except ImportError as e:
    PANDAS_TA_AVAILABLE = False
    logger.warning(f"pandas-ta not available: {e}")

try:
    import pywt
    PYWT_AVAILABLE = True
    logger.info("PyWavelets loaded successfully")
except ImportError as e:
    PYWT_AVAILABLE = False
    logger.warning(f"PyWavelets not available: {e}")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
    logger.info("LightGBM loaded successfully")
except ImportError as e:
    LIGHTGBM_AVAILABLE = False
    logger.warning(f"LightGBM not available: {e}")

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
    logger.info("XGBoost loaded successfully")
except ImportError as e:
    XGBOOST_AVAILABLE = False
    logger.warning(f"XGBoost not available: {e}")

try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
    logger.info("Prophet loaded successfully")
except ImportError as e:
    PROPHET_AVAILABLE = False
    logger.warning(f"Prophet not available: {e}")

try:
    from statsmodels.tsa.arima.model import ARIMA
    STATSMODELS_AVAILABLE = True
    logger.info("Statsmodels loaded successfully")
except ImportError as e:
    STATSMODELS_AVAILABLE = False
    logger.warning(f"Statsmodels not available: {e}")

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ùˆ Ù…ØµÙˆØ±Ø³Ø§Ø²ÛŒ
from scipy.signal import find_peaks
from scipy.stats import pearsonr
import matplotlib.pyplot as plt

# Ù…Ø¯ÛŒØ±ÛŒØª Ø´Ø±Ø·ÛŒ Ø¨Ø±Ø§ÛŒ seaborn
try:
    import seaborn as sns
    SEABORN_AVAILABLE = True
    logger.info("Seaborn loaded successfully")
except ImportError as e:
    SEABORN_AVAILABLE = False
    logger.warning(f"Seaborn not available: {e}")

class AdvancedTradingBot:
    def __init__(self):
        # Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡
        self.conn = sqlite3.connect('trading_bot.db', check_same_thread=False)
        self.create_tables()
        
        # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
        self.models = self.initialize_models()
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        self.exchange = ccxt.binance()
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
        if TORCH_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained("HooshvareLab/gpt2-persian")
                self.model = AutoModelForCausalLM.from_pretrained("HooshvareLab/gpt2-persian")
                self.sentiment_pipeline = pipeline("sentiment-analysis", model="HooshvareLab/bert-fa-sentiment-deepsenti")
                logger.info("Language models loaded successfully")
            except Exception as e:
                logger.error(f"Error loading language models: {e}")
                self.tokenizer = None
                self.model = None
                self.sentiment_pipeline = None
        else:
            self.tokenizer = None
            self.model = None
            self.sentiment_pipeline = None
    
    def create_tables(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯Ø§ÙˆÙ„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        cursor = self.conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS users (
            user_id INTEGER PRIMARY KEY,
            username TEXT,
            first_name TEXT,
            language TEXT DEFAULT 'fa',
            preferences TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS analyses (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER,
            symbol TEXT,
            analysis_type TEXT,
            result TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (user_id) REFERENCES users (user_id)
        )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS signals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER,
            symbol TEXT,
            signal_type TEXT,
            signal_value TEXT,
            confidence REAL,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (user_id) REFERENCES users (user_id)
        )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS watchlist (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER,
            symbol TEXT,
            added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (user_id) REFERENCES users (user_id)
        )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS performance (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER,
            symbol TEXT,
            strategy TEXT,
            entry_price REAL,
            exit_price REAL,
            profit_loss REAL,
            duration INTEGER,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (user_id) REFERENCES users (user_id)
        )
        ''')
        
        self.conn.commit()
        logger.info("Database tables created successfully")
    
    def initialize_models(self):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„"""
        models = {
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'svm': SVR(kernel='rbf', C=50, gamma=0.1),
            'knn': KNeighborsRegressor(n_neighbors=5),
            'linear_regression': LinearRegression(),
        }
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        if XGBOOST_AVAILABLE:
            models['xgboost'] = xgb.XGBRegressor(n_estimators=100, random_state=42)
        
        if LIGHTGBM_AVAILABLE:
            models['lightgbm'] = lgb.LGBMRegressor(n_estimators=100, random_state=42)
        
        if PROPHET_AVAILABLE:
            models['prophet'] = Prophet()
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
        if TF_AVAILABLE:
            models['lstm'] = self.build_lstm_model()
            models['gru'] = self.build_gru_model()
        
        logger.info("Machine learning models initialized")
        return models
    
    def build_lstm_model(self):
        """Ø³Ø§Ø®Øª Ù…Ø¯Ù„ LSTM"""
        if not TF_AVAILABLE:
            return None
            
        model = Sequential([
            LSTM(50, return_sequences=True, input_shape=(60, 5)),
            Dropout(0.2),
            LSTM(50),
            Dropout(0.2),
            Dense(25),
            Dense(1)
        ])
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
        return model
    
    def build_gru_model(self):
        """Ø³Ø§Ø®Øª Ù…Ø¯Ù„ GRU"""
        if not TF_AVAILABLE:
            return None
            
        model = Sequential([
            GRU(50, return_sequences=True, input_shape=(60, 5)),
            Dropout(0.2),
            GRU(50),
            Dropout(0.2),
            Dense(25),
            Dense(1)
        ])
        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
        return model
    
    def advanced_elliott_wave(self, data):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù…ÙˆØ§Ø¬ Ø§Ù„ÛŒÙˆØª Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        close_prices = data['Close'].values
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ¨Ø¯ÛŒÙ„ ÙˆÛŒÙˆÙ„Øª Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
        if PYWT_AVAILABLE:
            try:
                coeffs = pywt.wavedec(close_prices, 'db1', level=5)
            except Exception as e:
                logger.error(f"Error in wavelet transform: {e}")
                coeffs = None
        else:
            coeffs = None
        
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù‚Ù„Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¯Ø±Ù‡â€ŒÙ‡Ø§
        peaks, _ = find_peaks(close_prices, distance=5)
        troughs, _ = find_peaks(-close_prices, distance=5)
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ù…ÙˆØ§Ø¬
        waves = []
        for i in range(1, len(peaks)):
            if peaks[i] > peaks[i-1] and troughs[i] > troughs[i-1]:
                waves.append({
                    'type': 'impulse',
                    'start': troughs[i-1],
                    'end': peaks[i],
                    'strength': (close_prices[peaks[i]] - close_prices[troughs[i-1]]) / close_prices[troughs[i-1]]
                })
            elif peaks[i] < peaks[i-1] and troughs[i] < troughs[i-1]:
                waves.append({
                    'type': 'corrective',
                    'start': peaks[i-1],
                    'end': troughs[i],
                    'strength': (close_prices[peaks[i-1]] - close_prices[troughs[i]]) / close_prices[troughs[i]]
                })
        
        return {
            'waves': waves[-10:],  # 10 Ù…ÙˆØ¬ Ø¢Ø®Ø±
            'current_pattern': 'bullish' if len([w for w in waves if w['type'] == 'impulse']) > 5 else 'bearish',
            'wavelet_coeffs': coeffs,
            'dominant_cycle': self.detect_dominant_cycle(close_prices)
        }
    
    def detect_dominant_cycle(self, prices):
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú†Ø±Ø®Ù‡ ØºØ§Ù„Ø¨ Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ ÙÙˆØ±ÛŒÙ‡"""
        try:
            fft = np.fft.fft(prices)
            freqs = np.fft.fftfreq(len(prices))
            dominant_freq = freqs[np.argmax(np.abs(fft[1:])) + 1]
            return 1 / dominant_freq if dominant_freq != 0 else 0
        except Exception as e:
            logger.error(f"Error in dominant cycle detection: {e}")
            return 0
    
    def advanced_supply_demand(self, symbol):
        """ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¹Ø±Ø¶Ù‡ Ùˆ ØªÙ‚Ø§Ø¶Ø§"""
        try:
            orderbook = self.exchange.fetch_order_book(symbol, limit=50)
            
            # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù‚ Ø¨Ø§Ø²Ø§Ø±
            bids = orderbook['bids']
            asks = orderbook['asks']
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø±Ø²Ø´ (Value Area)
            total_volume = sum([bid[1] for bid in bids] + [ask[1] for ask in asks])
            value_area_high = None
            value_area_low = None
            cumulative_volume = 0
            
            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø±Ø²Ø´ (70% Ø­Ø¬Ù…)
            for level in sorted(bids + asks, key=lambda x: x[0]):
                cumulative_volume += level[1]
                if cumulative_volume >= total_volume * 0.7:
                    value_area_high = level[0]
                    break
            
            cumulative_volume = 0
            for level in sorted(bids + asks, key=lambda x: x[0], reverse=True):
                cumulative_volume += level[1]
                if cumulative_volume >= total_volume * 0.7:
                    value_area_low = level[0]
                    break
            
            # ØªØ­Ù„ÛŒÙ„ Ù†Ù‚Ø§Ø· Ú©Ù†ØªØ±Ù„ (Point of Control)
            poc = max(bids + asks, key=lambda x: x[1])[0]
            
            # ØªØ­Ù„ÛŒÙ„ Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ø³ÙØ§Ø±Ø´Ø§Øª
            bid_volume = sum([bid[1] for bid in bids])
            ask_volume = sum([ask[1] for ask in asks])
            imbalance = (bid_volume - ask_volume) / (bid_volume + ask_volume) if (bid_volume + ask_volume) > 0 else 0
            
            # ØªØ­Ù„ÛŒÙ„ Ù„ÛŒÚ©ÙˆÛŒÛŒØ¯ÛŒØªÛŒ
            liquidity_score = self.calculate_liquidity_score(bids, asks)
            
            return {
                'value_area': {'high': value_area_high, 'low': value_area_low},
                'point_of_control': poc,
                'imbalance': imbalance,
                'liquidity_score': liquidity_score,
                'bid_levels': bids[:10],
                'ask_levels': asks[:10],
                'market_depth': self.calculate_market_depth(bids, asks)
            }
        except Exception as e:
            logger.error(f"Error in supply demand analysis: {e}")
            return {'error': str(e)}
    
    def calculate_liquidity_score(self, bids, asks):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‚Ø¯ÛŒÙ†Ú¯ÛŒ"""
        try:
            spread = asks[0][0] - bids[0][0]
            depth_1percent = sum([bid[1] for bid in bids if bid[0] >= bids[0][0] * 0.99]) + \
                           sum([ask[1] for ask in asks if ask[0] <= asks[0][0] * 1.01])
            
            spread_score = 1 / (1 + spread)
            depth_score = depth_1percent / 1000000
            
            return (spread_score + depth_score) / 2
        except Exception as e:
            logger.error(f"Error in liquidity score calculation: {e}")
            return 0.5
    
    def calculate_market_depth(self, bids, asks):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¹Ù…Ù‚ Ø¨Ø§Ø²Ø§Ø±"""
        try:
            depth = {}
            for percent in [0.1, 0.5, 1, 2, 5]:
                bid_depth = sum([bid[1] for bid in bids if bid[0] >= bids[0][0] * (1 - percent/100)])
                ask_depth = sum([ask[1] for ask in asks if ask[0] <= asks[0][0] * (1 + percent/100)])
                depth[f'{percent}%'] = {'bid': bid_depth, 'ask': ask_depth}
            return depth
        except Exception as e:
            logger.error(f"Error in market depth calculation: {e}")
            return {}
    
    def advanced_technical_analysis(self, data):
        """ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù…ØªØ¹Ø¯Ø¯"""
        try:
            # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
            ichimoku = self.calculate_ichimoku(data)
            macd = self.calculate_macd(data)
            rsi = self.calculate_rsi(data)
            stoch = self.calculate_stochastic(data)
            bb = self.calculate_bollinger_bands(data)
            atr = self.calculate_atr(data)
            vwap = self.calculate_vwap(data)
            
            # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªØ§Ù„Ø§Ø¨ Ø¨Ø§ pandas-ta Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ TA-Lib
            williams_r = None
            cci = None
            
            if TALIB_AVAILABLE:
                try:
                    williams_r = talib.WILLR(data['High'], data['Low'], data['Close'], timeperiod=14)
                    cci = talib.CCI(data['High'], data['Low'], data['Close'], timeperiod=14)
                except Exception as e:
                    logger.error(f"Error calculating TA-Lib indicators: {e}")
            elif PANDAS_TA_AVAILABLE:
                try:
                    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pandas-ta Ø¨Ù‡ Ø¬Ø§ÛŒ TA-Lib
                    df = data.copy()
                    df.ta.williams_r(length=14, append=True)
                    df.ta.cci(length=14, append=True)
                    williams_r = df['WILLR_14'].values
                    cci = df['CCI_14'].values
                except Exception as e:
                    logger.error(f"Error calculating pandas-ta indicators: {e}")
            
            return {
                'ichimoku': ichimoku,
                'macd': macd,
                'rsi': rsi,
                'stochastic': stoch,
                'bollinger': bb,
                'atr': atr,
                'vwap': vwap,
                'williams_r': williams_r[-1] if williams_r is not None else None,
                'cci': cci[-1] if cci is not None else None
            }
        except Exception as e:
            logger.error(f"Error in technical analysis: {e}")
            return {}
    
    def calculate_ichimoku(self, data):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø§ÛŒÚ†ÛŒÙ…ÙˆÚ©Ùˆ"""
        try:
            high_9 = data['High'].rolling(window=9).max()
            low_9 = data['Low'].rolling(window=9).min()
            high_26 = data['High'].rolling(window=26).max()
            low_26 = data['Low'].rolling(window=26).min()
            high_52 = data['High'].rolling(window=52).max()
            low_52 = data['Low'].rolling(window=52).min()
            
            tenkan_sen = (high_9 + low_9) / 2
            kijun_sen = (high_26 + low_26) / 2
            senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(26)
            senkou_span_b = ((high_52 + low_52) / 2).shift(26)
            chikou_span = data['Close'].shift(-26)
            
            return {
                'tenkan_sen': tenkan_sen.iloc[-1],
                'kijun_sen': kijun_sen.iloc[-1],
                'senkou_span_a': senkou_span_a.iloc[-1],
                'senkou_span_b': senkou_span_b.iloc[-1],
                'chikou_span': chikou_span.iloc[-1],
                'cloud_bullish': senkou_span_a.iloc[-1] > senkou_span_b.iloc[-1]
            }
        except Exception as e:
            logger.error(f"Error calculating Ichimoku: {e}")
            return {}
    
    def calculate_macd(self, data):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± MACD"""
        try:
            exp12 = data['Close'].ewm(span=12, adjust=False).mean()
            exp26 = data['Close'].ewm(span=26, adjust=False).mean()
            macd = exp12 - exp26
            signal = macd.ewm(span=9, adjust=False).mean()
            histogram = macd - signal
            
            return {
                'macd': macd.iloc[-1],
                'signal': signal.iloc[-1],
                'histogram': histogram.iloc[-1]
            }
        except Exception as e:
            logger.error(f"Error calculating MACD: {e}")
            return {}
    
    def calculate_rsi(self, data):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± RSI"""
        try:
            delta = data['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi.iloc[-1]
        except Exception as e:
            logger.error(f"Error calculating RSI: {e}")
            return 50
    
    def calculate_stochastic(self, data):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Stochastic"""
        try:
            low_14 = data['Low'].rolling(window=14).min()
            high_14 = data['High'].rolling(window=14).max()
            k_percent = 100 * ((data['Close'] - low_14) / (high_14 - low_14))
            d_percent = k_percent.rolling(window=3).mean()
            
            return {
                'stoch_k': k_percent.iloc[-1],
                'stoch_d': d_percent.iloc[-1]
            }
        except Exception as e:
            logger.error(f"Error calculating Stochastic: {e}")
            return {'stoch_k': 50, 'stoch_d': 50}
    
    def calculate_bollinger_bands(self, data):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø¨ÙˆÙ„ÛŒÙ†Ú¯Ø± Ø¨Ø§Ù†Ø¯"""
        try:
            ma_20 = data['Close'].rolling(window=20).mean()
            std_20 = data['Close'].rolling(window=20).std()
            upper_band = ma_20 + (std_20 * 2)
            lower_band = ma_20 - (std_20 * 2)
            
            position = "above_upper" if data['Close'].iloc[-1] > upper_band.iloc[-1] else \
                      "below_lower" if data['Close'].iloc[-1] < lower_band.iloc[-1] else "inside"
            
            return {
                'upper': upper_band.iloc[-1],
                'middle': ma_20.iloc[-1],
                'lower': lower_band.iloc[-1],
                'position': position
            }
        except Exception as e:
            logger.error(f"Error calculating Bollinger Bands: {e}")
            return {}
    
    def calculate_atr(self, data):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± ATR"""
        try:
            high_low = data['High'] - data['Low']
            high_close = np.abs(data['High'] - data['Close'].shift())
            low_close = np.abs(data['Low'] - data['Close'].shift())
            ranges = pd.concat([high_low, high_close, low_close], axis=1)
            true_range = ranges.max(axis=1)
            atr = true_range.rolling(window=14).mean()
            return atr.iloc[-1]
        except Exception as e:
            logger.error(f"Error calculating ATR: {e}")
            return 0
    
    def calculate_vwap(self, data):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± VWAP"""
        try:
            q = data['Volume']
            p = data['Close']
            vwap = (p * q).cumsum() / q.cumsum()
            return vwap.iloc[-1]
        except Exception as e:
            logger.error(f"Error calculating VWAP: {e}")
            return data['Close'].iloc[-1]
    
    def advanced_sentiment_analysis(self, symbol):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ú†Ù†Ø¯ Ù…Ù†Ø¨Ø¹"""
        sentiment_scores = {
            'news': 0,
            'social_media': 0,
            'analyst_ratings': 0,
            'options_market': 0
        }
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ø®Ø¨Ø§Ø±
        news_sentiment = self.analyze_news_sentiment(symbol)
        sentiment_scores['news'] = news_sentiment
        
        # ØªØ­Ù„ÛŒÙ„ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ
        social_sentiment = self.analyze_social_media_sentiment(symbol)
        sentiment_scores['social_media'] = social_sentiment
        
        # ØªØ­Ù„ÛŒÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù†
        analyst_sentiment = self.analyze_analyst_ratings(symbol)
        sentiment_scores['analyst_ratings'] = analyst_sentiment
        
        # ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Ø²Ø§Ø± Ø§Ø®ØªÛŒØ§Ø±Ø§Øª
        options_sentiment = self.analyze_options_sentiment(symbol)
        sentiment_scores['options_market'] = options_sentiment
        
        # ØªØ±Ú©ÛŒØ¨ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª
        combined_sentiment = np.mean(list(sentiment_scores.values()))
        
        return {
            'scores': sentiment_scores,
            'combined': combined_sentiment,
            'interpretation': self.interpret_sentiment(combined_sentiment)
        }
    
    def analyze_news_sentiment(self, symbol):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø®Ø¨Ø§Ø±"""
        try:
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            news_headlines = [
                f"{symbol} Ø¯Ø± Ø­Ø§Ù„ Ø«Ø¨Øª Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª",
                f"ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØµØ¹ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ {symbol} Ø¯Ø§Ø±Ù†Ø¯",
                f"Ù†Ú¯Ø±Ø§Ù†ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø±Ú¯ÙˆÙ„Ø§ØªÙˆØ±ÛŒ {symbol} Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡"
            ]
            
            sentiments = []
            for headline in news_headlines:
                if self.sentiment_pipeline:
                    try:
                        result = self.sentiment_pipeline(headline)[0]
                        sentiments.append(result['score'] if result['label'] == 'POSITIVE' else -result['score'])
                    except Exception as e:
                        logger.error(f"Error in sentiment analysis: {e}")
                        sentiments.append(np.random.uniform(-1, 1))
                else:
                    # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² Ø§Ù…ØªÛŒØ§Ø² ØªØµØ§Ø¯ÙÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    sentiments.append(np.random.uniform(-1, 1))
            
            return np.mean(sentiments)
        except Exception as e:
            logger.error(f"Error in news sentiment analysis: {e}")
            return 0
    
    def analyze_social_media_sentiment(self, symbol):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ"""
        try:
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            tweets = [
                f"Ù…Ù† Ø¨Ù‡ {symbol} Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø¨ÛŒÙ† Ù‡Ø³ØªÙ…! ğŸš€",
                f"{symbol} Ø¯Ø± Ø­Ø§Ù„ Ø³Ù‚ÙˆØ· Ø§Ø³ØªØŒ Ø¨ÙØ±ÙˆØ´ÛŒØ¯!",
                f"ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ {symbol} Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ø§Ø³Øª"
            ]
            
            sentiments = []
            for tweet in tweets:
                if self.sentiment_pipeline:
                    try:
                        result = self.sentiment_pipeline(tweet)[0]
                        sentiments.append(result['score'] if result['label'] == 'POSITIVE' else -result['score'])
                    except Exception as e:
                        logger.error(f"Error in sentiment analysis: {e}")
                        sentiments.append(np.random.uniform(-1, 1))
                else:
                    sentiments.append(np.random.uniform(-1, 1))
            
            return np.mean(sentiments)
        except Exception as e:
            logger.error(f"Error in social media sentiment analysis: {e}")
            return 0
    
    def analyze_analyst_ratings(self, symbol):
        """ØªØ­Ù„ÛŒÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ØªØ­Ù„ÛŒÙ„Ú¯Ø±Ø§Ù†"""
        try:
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            ratings = [
                {'rating': 'BUY', 'weight': 1},
                {'rating': 'HOLD', 'weight': 0},
                {'rating': 'SELL', 'weight': -1}
            ]
            
            weighted_score = sum(r['weight'] for r in ratings) / len(ratings)
            return weighted_score
        except Exception as e:
            logger.error(f"Error in analyst ratings analysis: {e}")
            return 0
    
    def analyze_options_sentiment(self, symbol):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø± Ø§Ø®ØªÛŒØ§Ø±Ø§Øª"""
        try:
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            put_volume = 10000
            call_volume = 15000
            put_call_ratio = put_volume / call_volume
            
            return 1 - put_call_ratio
        except Exception as e:
            logger.error(f"Error in options sentiment analysis: {e}")
            return 0
    
    def interpret_sentiment(self, score):
        """ØªÙØ³ÛŒØ± Ø§Ù…ØªÛŒØ§Ø² Ø§Ø­Ø³Ø§Ø³Ø§Øª"""
        if score > 0.3:
            return "Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø³ÛŒØ§Ø± Ù…Ø«Ø¨Øª - Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø¯Ø§Ù…Ù‡ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ"
        elif score > 0.1:
            return "Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…Ø«Ø¨Øª - Ø´Ø±Ø§ÛŒØ· Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø±Ø§ÛŒ Ø±Ø´Ø¯"
        elif score > -0.1:
            return "Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø®Ù†Ø«ÛŒ - Ø¨Ø§Ø²Ø§Ø± Ø¯Ø± Ø­Ø§Ù„Øª Ø§Ù†ØªØ¸Ø§Ø±"
        elif score > -0.3:
            return "Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…Ù†ÙÛŒ - Ø§Ø­ØªÙ…Ø§Ù„ Ø§ØµÙ„Ø§Ø­ Ù‚ÛŒÙ…Øª"
        else:
            return "Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø³ÛŒØ§Ø± Ù…Ù†ÙÛŒ - Ù‡Ø´Ø¯Ø§Ø± Ø±ÛŒØ²Ø´ Ø´Ø¯ÛŒØ¯"
    
    def generate_persian_explanation(self, analysis_data):
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØ¶ÛŒØ­Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
        if self.model is None or self.tokenizer is None:
            # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª Ø«Ø§Ø¨Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            return f"""
ğŸ“Š *ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ {analysis_data['symbol']}*

ğŸ’° *Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ:* {analysis_data['price']:,.2f} USD

ğŸ¤– *Ø³ÛŒÚ¯Ù†Ø§Ù„:* {analysis_data['signal']}
ğŸ“ˆ *Ø§Ø·Ù…ÛŒÙ†Ø§Ù†:* {analysis_data['confidence']*100:.1f}%

ğŸ“ˆ *ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„:*
â€¢ Ø§Ù„Ú¯ÙˆÛŒ Ø§Ù„Ø¨Ø±ÙˆÚ©Ø³: {analysis_data['elliott']['current_pattern']}
â€¢ Ø§ÛŒÚ†ÛŒÙ…ÙˆÚ©Ùˆ: {'ØµØ¹ÙˆØ¯ÛŒ' if analysis_data['technical'].get('ichimoku', {}).get('cloud_bullish', False) else 'Ù†Ø²ÙˆÙ„ÛŒ'}
â€¢ RSI: {analysis_data['technical'].get('rsi', 50):.1f}
â€¢ MACD: {analysis_data['technical'].get('macd', {}).get('histogram', 0):.2f}

âš–ï¸ *Ø¹Ø±Ø¶Ù‡ Ùˆ ØªÙ‚Ø§Ø¶Ø§:*
â€¢ Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø±Ø²Ø´: {analysis_data['supply_demand'].get('value_area', {}).get('low', 0):.2f} - {analysis_data['supply_demand'].get('value_area', {}).get('high', 0):.2f}
â€¢ Ù†Ù‚Ø·Ù‡ Ú©Ù†ØªØ±Ù„: {analysis_data['supply_demand'].get('point_of_control', 0):.2f}
â€¢ Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„: {analysis_data['supply_demand'].get('imbalance', 0)*100:.1f}%

ğŸ“° *ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª:* {analysis_data['sentiment']['interpretation']}

âš ï¸ *Ù‡Ø´Ø¯Ø§Ø±:* Ø§ÛŒÙ† ØªØ­Ù„ÛŒÙ„ ØµØ±ÙØ§Ù‹ Ø¬Ù†Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø§Ø±Ø¯. Ù‡Ù…ÛŒØ´Ù‡ Ø±ÛŒØ³Ú© Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒØ¯.
            """
        
        try:
            prompt = f"""
            Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù…Ø§Ù„ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù‡Ø³ØªÛŒØ¯. Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§Ù…Ù„ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯:
            
            Ù†Ù…Ø§Ø¯: {analysis_data['symbol']}
            Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ: {analysis_data['price']}
            Ø³ÛŒÚ¯Ù†Ø§Ù„: {analysis_data['signal']}
            Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {analysis_data['confidence']*100:.1f}%
            
            ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„:
            - Ø§Ù„Ú¯ÙˆÛŒ Ø§Ù„Ø¨Ø±ÙˆÚ©Ø³: {analysis_data['elliott']['current_pattern']}
            - Ù…Ù†Ø§Ø·Ù‚ Ø¹Ø±Ø¶Ù‡ Ùˆ ØªÙ‚Ø§Ø¶Ø§: {analysis_data['supply_demand']['point_of_control']}
            - Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ: RSI={analysis_data['technical']['rsi']:.1f}, MACD={analysis_data['technical']['macd']['histogram']:.2f}
            
            ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª: {analysis_data['sentiment']['interpretation']}
            
            Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø§Ù…Ù„:
            1. ØªÙˆØ¶ÛŒØ­ ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø¨Ø§Ø²Ø§Ø±
            2. Ø¯Ù„Ø§ÛŒÙ„ Ø³ÛŒÚ¯Ù†Ø§Ù„ ØµØ§Ø¯Ø± Ø´Ø¯Ù‡
            3. Ù†Ù‚Ø§Ø· ÙˆØ±ÙˆØ¯ Ùˆ Ø®Ø±ÙˆØ¬ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
            4. Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú© Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
            5. Ú†Ø´Ù…â€ŒØ§Ù†Ø¯Ø§Ø² Ø¢ÛŒÙ†Ø¯Ù‡
            """
            
            inputs = self.tokenizer(prompt, return_tensors="pt")
            outputs = self.model.generate(
                inputs.input_ids,
                max_length=1000,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True
            )
            
            explanation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return explanation
        except Exception as e:
            logger.error(f"Error generating Persian explanation: {e}")
            return f"ØªØ­Ù„ÛŒÙ„ {analysis_data['symbol']}: Ø³ÛŒÚ¯Ù†Ø§Ù„ {analysis_data['signal']} Ø¨Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† {analysis_data['confidence']*100:.1f}%"
    
    def save_analysis(self, user_id, symbol, analysis_type, result):
        """Ø°Ø®ÛŒØ±Ù‡ ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
            INSERT INTO analyses (user_id, symbol, analysis_type, result)
            VALUES (?, ?, ?, ?)
            ''', (user_id, symbol, analysis_type, json.dumps(result)))
            self.conn.commit()
            return cursor.lastrowid
        except Exception as e:
            logger.error(f"Error saving analysis: {e}")
            return None
    
    def save_signal(self, user_id, symbol, signal_type, signal_value, confidence):
        """Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
            INSERT INTO signals (user_id, symbol, signal_type, signal_value, confidence)
            VALUES (?, ?, ?, ?, ?)
            ''', (user_id, symbol, signal_type, signal_value, confidence))
            self.conn.commit()
            return cursor.lastrowid
        except Exception as e:
            logger.error(f"Error saving signal: {e}")
            return None
    
    def get_user_watchlist(self, user_id):
        """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ú©Ø§Ø±Ø¨Ø±"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('SELECT symbol FROM watchlist WHERE user_id = ?', (user_id,))
            return [row[0] for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"Error getting watchlist: {e}")
            return []
    
    def add_to_watchlist(self, user_id, symbol):
        """Ø§ÙØ²ÙˆØ¯Ù† Ù†Ù…Ø§Ø¯ Ø¨Ù‡ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('INSERT INTO watchlist (user_id, symbol) VALUES (?, ?)', (user_id, symbol))
            self.conn.commit()
        except Exception as e:
            logger.error(f"Error adding to watchlist: {e}")
    
    def remove_from_watchlist(self, user_id, symbol):
        """Ø­Ø°Ù Ù†Ù…Ø§Ø¯ Ø§Ø² ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('DELETE FROM watchlist WHERE user_id = ? AND symbol = ?', (user_id, symbol))
            self.conn.commit()
        except Exception as e:
            logger.error(f"Error removing from watchlist: {e}")
    
    def generate_performance_report(self, user_id):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ø§Ø±Ø¨Ø±"""
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
            SELECT symbol, strategy, profit_loss, timestamp
            FROM performance
            WHERE user_id = ?
            ORDER BY timestamp DESC
            ''', (user_id,))
            
            performance = []
            for row in cursor.fetchall():
                performance.append({
                    'symbol': row[0],
                    'strategy': row[1],
                    'profit_loss': row[2],
                    'timestamp': row[3]
                })
            
            if not performance:
                return "Ù‡ÛŒÚ† Ù…Ø¹Ø§Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø«Ø¨Øª Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª."
            
            df = pd.DataFrame(performance)
            
            # Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¢Ù…Ø§Ø±ÛŒ
            total_trades = len(df)
            profitable_trades = len(df[df['profit_loss'] > 0])
            win_rate = profitable_trades / total_trades if total_trades > 0 else 0
            
            total_profit = df['profit_loss'].sum()
            avg_profit = df['profit_loss'].mean()
            max_profit = df['profit_loss'].max()
            max_loss = df['profit_loss'].min()
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø±Ù¾ ratio
            risk_free_rate = 0.02
            excess_returns = df['profit_loss'] - risk_free_rate/252
            sharpe_ratio = np.sqrt(252) * excess_returns.mean() / excess_returns.std() if len(excess_returns) > 1 else 0
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± Ø§ÙØª
            cumulative = (1 + df['profit_loss']).cumprod()
            peak = cumulative.expanding().max()
            drawdown = (cumulative - peak) / peak
            max_drawdown = drawdown.min()
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯
            plt.figure(figsize=(12, 6))
            if SEABORN_AVAILABLE:
                sns.set()  # ØªÙ†Ø¸ÛŒÙ… Ø§Ø³ØªØ§ÛŒÙ„ seaborn Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
            plt.plot(cumulative.index, cumulative.values, label='Cumulative Returns')
            plt.fill_between(cumulative.index, drawdown.values, 0, color='red', alpha=0.3, label='Drawdown')
            plt.title('Performance Chart')
            plt.xlabel('Date')
            plt.ylabel('Value')
            plt.legend()
            
            # ØªØ¨Ø¯ÛŒÙ„ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ù‡ base64
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png')
            buffer.seek(0)
            chart_base64 = base64.b64encode(buffer.read()).decode('utf-8')
            plt.close()
            
            report = f"""
ğŸ“Š *Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø´Ù…Ø§*

â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {total_trades}
â€¢ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø³ÙˆØ¯Ø¯Ù‡: {profitable_trades}
â€¢ Ù†Ø±Ø® Ø¨Ø±Ø¯: {win_rate:.1%}

â€¢ Ø³ÙˆØ¯ Ú©Ù„: {total_profit:.2f}%
â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø³ÙˆØ¯: {avg_profit:.2f}%
â€¢ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø³ÙˆØ¯: {max_profit:.2f}%
â€¢ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¶Ø±Ø±: {max_loss:.2f}%

â€¢ Ù†Ø³Ø¨Øª Ø´Ø§Ø±Ù¾: {sharpe_ratio:.2f}
â€¢ Ø­Ø¯Ø§Ú©Ø«Ø± Ø§ÙØª: {max_drawdown:.1%}

â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ: {df.groupby('strategy')['profit_loss'].mean().idxmax()}
            """
            
            return {
                'text': report,
                'chart': chart_base64
            }
        except Exception as e:
            logger.error(f"Error generating performance report: {e}")
            return "Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯"
    
    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ø¯Ø³ØªÙˆØ± Ø´Ø±ÙˆØ¹ Ø±Ø¨Ø§Øª"""
        keyboard = [
            [InlineKeyboardButton("ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚", callback_data='deep_analysis')],
            [InlineKeyboardButton("ğŸ”¥ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ù„Ø§ÛŒÛŒ", callback_data='golden_signals')],
            [InlineKeyboardButton("ğŸ“‹ Ù…Ø¯ÛŒØ±ÛŒØª ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª", callback_data='watchlist')],
            [InlineKeyboardButton("ğŸ“ˆ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯", callback_data='performance_report')],
            [InlineKeyboardButton("âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª", callback_data='settings')],
            [InlineKeyboardButton("â“ Ø±Ø§Ù‡Ù†Ù…Ø§", callback_data='help')]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await update.message.reply_text(
            "ğŸ¤– *Ø¨Ù‡ Ø±Ø¨Ø§Øª ØªØ±ÛŒØ¯ÛŒÙ†Ú¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯!*\\n\\nÙ„Ø·ÙØ§Ù‹ ÛŒÚ© Ú¯Ø²ÛŒÙ†Ù‡ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
            reply_markup=reply_markup,
            parse_mode='Markdown'
        )
    
    async def deep_analysis(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ù†Ù…Ø§Ø¯"""
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        context.user_data['state'] = 'deep_analysis'
        
        await query.edit_message_text(
            "Ù„Ø·ÙØ§Ù‹ Ù†Ù…Ø§Ø¯ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ù…Ø«Ø§Ù„: BTC-USD):"
        )
    
    async def handle_symbol(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù…Ø§Ø¯ ÙˆØ±ÙˆØ¯ÛŒ"""
        if context.user_data.get('state') == 'deep_analysis':
            symbol = update.message.text.upper()
            user_id = update.effective_user.id
            
            try:
                # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±
                data = yf.download(symbol, period="2y", interval="1d")
                
                if data.empty:
                    await update.message.reply_text("âŒ Ù†Ù…Ø§Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")
                    return
                
                # ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
                elliott = self.advanced_elliott_wave(data)
                supply_demand = self.advanced_supply_demand(symbol)
                technical = self.advanced_technical_analysis(data)
                sentiment = self.advanced_sentiment_analysis(symbol)
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø³ÛŒÚ¯Ù†Ø§Ù„
                signal_score = self.calculate_signal_score(elliott, supply_demand, technical, sentiment)
                signal = 'BUY' if signal_score > 0.7 else 'SELL' if signal_score < 0.3 else 'HOLD'
                
                # ØªØ­Ù„ÛŒÙ„ ØªØ±Ú©ÛŒØ¨ÛŒ
                analysis_result = {
                    'symbol': symbol,
                    'price': data['Close'].iloc[-1],
                    'elliott': elliott,
                    'supply_demand': supply_demand,
                    'technical': technical,
                    'sentiment': sentiment,
                    'signal': signal,
                    'confidence': signal_score,
                    'timestamp': datetime.now().isoformat()
                }
                
                # ØªÙˆÙ„ÛŒØ¯ ØªÙˆØ¶ÛŒØ­Ø§Øª ÙØ§Ø±Ø³ÛŒ
                explanation = self.generate_persian_explanation(analysis_result)
                
                # Ø°Ø®ÛŒØ±Ù‡ ØªØ­Ù„ÛŒÙ„
                self.save_analysis(user_id, symbol, 'deep_analysis', analysis_result)
                
                # Ø§Ø±Ø³Ø§Ù„ ØªØ­Ù„ÛŒÙ„
                await update.message.reply_text(explanation, parse_mode='Markdown')
                
            except Exception as e:
                logger.error(f"Error in deep analysis: {e}")
                await update.message.reply_text("âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")
            
            context.user_data['state'] = None
    
    def calculate_signal_score(self, elliott, supply_demand, technical, sentiment):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø³ÛŒÚ¯Ù†Ø§Ù„"""
        score = 0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø§Ù„ÛŒÙˆØª
        if elliott.get('current_pattern') == 'bullish':
            score += 0.3
        elif elliott.get('current_pattern') == 'bearish':
            score -= 0.3
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¹Ø±Ø¶Ù‡ Ùˆ ØªÙ‚Ø§Ø¶Ø§
        if supply_demand.get('imbalance') and supply_demand['imbalance'] > 0.2:  # ØªÙ‚Ø§Ø¶Ø§ÛŒ Ø¨ÛŒØ´ØªØ±
            score += 0.2
        elif supply_demand.get('imbalance') and supply_demand['imbalance'] < -0.2:  # Ø¹Ø±Ø¶Ù‡ Ø¨ÛŒØ´ØªØ±
            score -= 0.2
        
        # Ø§Ù…ØªÛŒØ§Ø² ØªÚ©Ù†ÛŒÚ©Ø§Ù„
        if technical.get('rsi') and technical['rsi'] < 30:  # Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´
            score += 0.2
        elif technical.get('rsi') and technical['rsi'] > 70:  # Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯
            score -= 0.2
        
        if technical.get('macd', {}).get('histogram', 0) > 0:
            score += 0.1
        else:
            score -= 0.1
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø§Ø­Ø³Ø§Ø³Ø§Øª
        score += sentiment.get('combined', 0) * 0.2
        
        return max(0, min(1, score))  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨ÛŒÙ† 0 Ùˆ 1
    
    async def golden_signals(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ù„Ø§ÛŒÛŒ"""
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        watchlist = self.get_user_watchlist(user_id)
        
        if not watchlist:
            await query.edit_message_text("âŒ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ø´Ù…Ø§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯.")
            return
        
        signals = []
        for symbol in watchlist:
            try:
                data = yf.download(symbol, period="6mo", interval="1d")
                
                # ØªØ­Ù„ÛŒÙ„ Ø³Ø±ÛŒØ¹
                elliott = self.advanced_elliott_wave(data)
                supply_demand = self.advanced_supply_demand(symbol)
                technical = self.advanced_technical_analysis(data)
                sentiment = self.advanced_sentiment_analysis(symbol)
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø³ÛŒÚ¯Ù†Ø§Ù„
                signal_score = self.calculate_signal_score(elliott, supply_demand, technical, sentiment)
                
                if signal_score > 0.8:  # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø·Ù„Ø§ÛŒÛŒ
                    signals.append({
                        'symbol': symbol,
                        'score': signal_score,
                        'price': data['Close'].iloc[-1],
                        'signal': 'BUY' if signal_score > 0.9 else 'SELL'
                    })
                    
            except Exception as e:
                logger.error(f"Error analyzing {symbol}: {e}")
        
        if signals:
            response = "ğŸ”¥ *Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ù„Ø§ÛŒÛŒ (Ø§Ø·Ù…ÛŒÙ†Ø§Ù† >80%):*\\n\\n"
            for sig in signals:
                response += f"â€¢ {sig['symbol']}: {sig['signal']}\\n"
                response += f"  Ù‚ÛŒÙ…Øª: {sig['price']:,.2f}\\n"
                response += f"  Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {sig['score']*100:.1f}%\\n\\n"
            
            await query.edit_message_text(response, parse_mode='Markdown')
        else:
            await query.edit_message_text("Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø·Ù„Ø§ÛŒÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
    
    async def watchlist_management(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù…Ø¯ÛŒØ±ÛŒØª ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª"""
        query = update.callback_query
        await query.answer()
        
        keyboard = [
            [InlineKeyboardButton("â• Ø§ÙØ²ÙˆØ¯Ù† Ù†Ù…Ø§Ø¯", callback_data='add_to_watchlist')],
            [InlineKeyboardButton("â– Ø­Ø°Ù Ù†Ù…Ø§Ø¯", callback_data='remove_from_watchlist')],
            [InlineKeyboardButton("ğŸ“‹ Ù†Ù…Ø§ÛŒØ´ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª", callback_data='show_watchlist')],
            [InlineKeyboardButton("ğŸ”™ Ø¨Ø§Ø²Ú¯Ø´Øª", callback_data='back_to_main')]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await query.edit_message_text(
            "ğŸ“‹ *Ù…Ø¯ÛŒØ±ÛŒØª ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª:*",
            reply_markup=reply_markup,
            parse_mode='Markdown'
        )
    
    async def add_to_watchlist(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ø§ÙØ²ÙˆØ¯Ù† Ù†Ù…Ø§Ø¯ Ø¨Ù‡ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª"""
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        context.user_data['state'] = 'add_to_watchlist'
        
        await query.edit_message_text("Ù„Ø·ÙØ§Ù‹ Ù†Ù…Ø§Ø¯ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:")
    
    async def remove_from_watchlist(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ø­Ø°Ù Ù†Ù…Ø§Ø¯ Ø§Ø² ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª"""
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        context.user_data['state'] = 'remove_from_watchlist'
        
        await query.edit_message_text("Ù„Ø·ÙØ§Ù‹ Ù†Ù…Ø§Ø¯ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:")
    
    async def show_watchlist(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù†Ù…Ø§ÛŒØ´ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª"""
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        watchlist = self.get_user_watchlist(user_id)
        
        if watchlist:
            response = "ğŸ“‹ *ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ø´Ù…Ø§:*\\n\\n"
            for i, symbol in enumerate(watchlist, 1):
                response += f"{i}. {symbol}\\n"
        else:
            response = "ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ø´Ù…Ø§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª."
        
        await query.edit_message_text(response, parse_mode='Markdown')
    
    async def handle_watchlist_action(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¹Ù…Ù„ÛŒØ§Øª ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª"""
        user_id = update.effective_user.id
        symbol = update.message.text.upper()
        state = context.user_data.get('state')
        
        if state == 'add_to_watchlist':
            self.add_to_watchlist(user_id, symbol)
            await update.message.reply_text(f"âœ… Ù†Ù…Ø§Ø¯ {symbol} Ø¨Ù‡ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")
        elif state == 'remove_from_watchlist':
            self.remove_from_watchlist(user_id, symbol)
            await update.message.reply_text(f"âœ… Ù†Ù…Ø§Ø¯ {symbol} Ø§Ø² ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ø­Ø°Ù Ø´Ø¯.")
        
        context.user_data['state'] = None
    
    async def performance_report(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        report = self.generate_performance_report(user_id)
        
        if isinstance(report, str):
            await query.edit_message_text(report)
        else:
            # Ø§Ø±Ø³Ø§Ù„ Ù†Ù…ÙˆØ¯Ø§Ø±
            try:
                chart_bytes = base64.b64decode(report['chart'])
                await context.bot.send_photo(
                    chat_id=update.effective_chat.id,
                    photo=chart_bytes,
                    caption="ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø´Ù…Ø§"
                )
                
                # Ø§Ø±Ø³Ø§Ù„ Ù…ØªÙ† Ú¯Ø²Ø§Ø±Ø´
                await query.edit_message_text(report['text'], parse_mode='Markdown')
            except Exception as e:
                logger.error(f"Error sending performance report: {e}")
                await query.edit_message_text("Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯")
    
    async def settings_menu(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù…Ù†ÙˆÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª"""
        query = update.callback_query
        await query.answer()
        
        keyboard = [
            [InlineKeyboardButton("ğŸŒ ØªØºÛŒÛŒØ± Ø²Ø¨Ø§Ù†", callback_data='change_language')],
            [InlineKeyboardButton("ğŸ”” Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø¹Ù„Ø§Ù†â€ŒÙ‡Ø§", callback_data='manage_notifications')],
            [InlineKeyboardButton("ğŸ”™ Ø¨Ø§Ø²Ú¯Ø´Øª", callback_data='back_to_main')]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await query.edit_message_text(
            "âš™ï¸ *ØªÙ†Ø¸ÛŒÙ…Ø§Øª:*",
            reply_markup=reply_markup,
            parse_mode='Markdown'
        )
    
    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ø¯Ø³ØªÙˆØ± Ø±Ø§Ù‡Ù†Ù…Ø§"""
        help_text = """
ğŸ¤– *Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø±Ø¨Ø§Øª ØªØ±ÛŒØ¯ÛŒÙ†Ú¯ Ù‡ÙˆØ´Ù…Ù†Ø¯*

â€¢ /start - Ø´Ø±ÙˆØ¹ Ø±Ø¨Ø§Øª Ùˆ Ù†Ù…Ø§ÛŒØ´ Ù…Ù†ÙˆÛŒ Ø§ØµÙ„ÛŒ
â€¢ /analyze [symbol] - ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ù†Ù…Ø§Ø¯
â€¢ /signals - Ù†Ù…Ø§ÛŒØ´ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ù„Ø§ÛŒÛŒ
â€¢ /watchlist - Ù…Ø¯ÛŒØ±ÛŒØª ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª
â€¢ /performance - Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯
â€¢ /settings - ØªÙ†Ø¸ÛŒÙ…Ø§Øª
â€¢ /help - Ù†Ù…Ø§ÛŒØ´ Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§

ğŸ“š *Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø±Ø¨Ø§Øª:*
- ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ 15+ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±
- ØªØ­Ù„ÛŒÙ„ Ø§Ù…ÙˆØ§Ø¬ Ø§Ù„ÛŒÙˆØª Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
- ØªØ­Ù„ÛŒÙ„ Ø¹Ø±Ø¶Ù‡ Ùˆ ØªÙ‚Ø§Ø¶Ø§ Ø¨Ø§ Ø¯ÙØªØ±Ú†Ù‡ Ø³ÙØ§Ø±Ø´Ø§Øª
- ØªØ±Ú©ÛŒØ¨ 10+ Ù…Ø¯Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†
- ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§Ø²Ø§Ø± Ø§Ø² Ú†Ù†Ø¯ Ù…Ù†Ø¨Ø¹
- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª Ø¨Ø§ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§
- Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú© Ù‡ÙˆØ´Ù…Ù†Ø¯
- Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¬Ø§Ù…Ø¹
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†

âš ï¸ *Ù‡Ø´Ø¯Ø§Ø±:* Ø§ÛŒÙ† Ø±Ø¨Ø§Øª ØµØ±ÙØ§Ù‹ Ø¬Ù†Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø§Ø±Ø¯ Ùˆ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø§ Ø´Ù…Ø§Ø³Øª.
        """
        
        await update.message.reply_text(help_text, parse_mode='Markdown')
    
    async def handle_callback(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Ù…Ø¯ÛŒØ±ÛŒØª callbackÙ‡Ø§"""
        query = update.callback_query
        await query.answer()
        
        data = query.data
        
        if data == 'deep_analysis':
            await self.deep_analysis(update, context)
        elif data == 'golden_signals':
            await self.golden_signals(update, context)
        elif data == 'watchlist':
            await self.watchlist_management(update, context)
        elif data == 'add_to_watchlist':
            await self.add_to_watchlist(update, context)
        elif data == 'remove_from_watchlist':
            await self.remove_from_watchlist(update, context)
        elif data == 'show_watchlist':
            await self.show_watchlist(update, context)
        elif data == 'performance_report':
            await self.performance_report(update, context)
        elif data == 'settings':
            await self.settings_menu(update, context)
        elif data == 'help':
            await self.help_command(update, context)
        elif data == 'back_to_main':
            await self.start(update, context)

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø±Ø¨Ø§Øª
    bot = AdvancedTradingBot()
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªÙ„Ú¯Ø±Ø§Ù…
    application = Application.builder().token(os.getenv('TELEGRAM_TOKEN')).build()
    
    # Ø§ÙØ²ÙˆØ¯Ù† Ù‡Ù†Ø¯Ù„Ø±Ù‡Ø§
    application.add_handler(CommandHandler("start", bot.start))
    application.add_handler(CommandHandler("help", bot.help_command))
    application.add_handler(CommandHandler("analyze", bot.handle_symbol))
    application.add_handler(CommandHandler("signals", bot.golden_signals))
    application.add_handler(CommandHandler("watchlist", bot.watchlist_management))
    application.add_handler(CommandHandler("performance", bot.performance_report))
    application.add_handler(CommandHandler("settings", bot.settings_menu))
    application.add_handler(CallbackQueryHandler(bot.handle_callback))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, bot.handle_symbol))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, bot.handle_watchlist_action))
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ø±Ø¨Ø§Øª
    application.run_polling()

if __name__ == '__main__':
    main()